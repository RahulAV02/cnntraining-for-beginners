{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "b-d7fh5vThPg",
        "outputId": "bd0fd925-8865-4be6-9e60-2abcaee31c87"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'my_include'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-b40034b71ae8>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Assuming 'my_include' directory is in the same directory as this Colab notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmy_include\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_data_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmy_include\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'my_include'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from time import time\n",
        "import math\n",
        "\n",
        "# Assuming 'my_include' directory is in the same directory as this Colab notebook\n",
        "from my_include.data import get_data_set\n",
        "from my_include.model import model, lr\n",
        "\n",
        "# ... (rest of your code remains unchanged)\n",
        "\n",
        "train_x, train_y = get_data_set(\"train\")\n",
        "test_x, test_y = get_data_set(\"test\")\n",
        "tf.set_random_seed(21)\n",
        "x, y, output, y_pred_cls, global_step, learning_rate = model()\n",
        "global_accuracy = 0\n",
        "epoch_start = 0\n",
        "\n",
        "# PARAMS\n",
        "_BATCH_SIZE = 128\n",
        "_EPOCH = 10\n",
        "_SAVE_PATH = \"./tensorboard/cifar-10-v1.0.0/\"\n",
        "\n",
        "# LOSS AND OPTIMIZER\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=output, labels=y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
        "                                   beta1=0.9,\n",
        "                                   beta2=0.999,\n",
        "                                   epsilon=1e-08).minimize(loss, global_step=global_step)\n",
        "\n",
        "# PREDICTION AND ACCURACY CALCULATION\n",
        "correct_prediction = tf.equal(y_pred_cls, tf.argmax(y, axis=1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "# SAVER\n",
        "merged = tf.summary.merge_all()\n",
        "saver = tf.train.Saver()\n",
        "sess = tf.Session()\n",
        "train_writer = tf.summary.FileWriter(_SAVE_PATH, sess.graph)\n",
        "\n",
        "try:\n",
        "    print(\"Trying to restore the last checkpoint...\")\n",
        "    last_chk_path = tf.train.latest_checkpoint(checkpoint_dir=_SAVE_PATH)\n",
        "    saver.restore(sess, save_path=last_chk_path)\n",
        "    print(\"Restored checkpoint from:\", last_chk_path)\n",
        "except ValueError:\n",
        "    print(\"Failed to restore checkpoint. Initializing variables instead.\")\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "def train(epoch):\n",
        "    global epoch_start\n",
        "    epoch_start = time()\n",
        "    batch_size = int(math.ceil(len(train_x) / _BATCH_SIZE))\n",
        "    i_global = 0\n",
        "\n",
        "    for s in range(batch_size):\n",
        "        batch_xs = train_x[s*_BATCH_SIZE: (s+1)*_BATCH_SIZE]\n",
        "        batch_ys = train_y[s*_BATCH_SIZE: (s+1)*_BATCH_SIZE]\n",
        "\n",
        "        start_time = time()\n",
        "        i_global, _, batch_loss, batch_acc = sess.run(\n",
        "            [global_step, optimizer, loss, accuracy],\n",
        "            feed_dict={x: batch_xs, y: batch_ys, learning_rate: lr(epoch)})\n",
        "        duration = time() - start_time\n",
        "\n",
        "        if s % 10 == 0:\n",
        "            percentage = int(round((s/batch_size)*100))\n",
        "            bar_len = 29\n",
        "            filled_len = int((bar_len*int(percentage))/100)\n",
        "            bar = '=' * filled_len + '>' + '-' * (bar_len - filled_len)\n",
        "            msg = \"Global step: {:>5} - [{}] {:>3}% - acc: {:.4f} - loss: {:.4f} - {:.1f} sample/sec\"\n",
        "            print(msg.format(i_global, bar, percentage, batch_acc, batch_loss, _BATCH_SIZE / duration))\n",
        "\n",
        "    test_and_save(i_global, epoch)\n",
        "\n",
        "def test_and_save(_global_step, epoch):\n",
        "    global global_accuracy\n",
        "    global epoch_start\n",
        "    i = 0\n",
        "    predicted_class = np.zeros(shape=len(test_x), dtype=np.int)\n",
        "    while i < len(test_x):\n",
        "        j = min(i + _BATCH_SIZE, len(test_x))\n",
        "        batch_xs = test_x[i:j, :]\n",
        "        batch_ys = test_y[i:j, :]\n",
        "        predicted_class[i:j] = sess.run(\n",
        "            y_pred_cls, feed_dict={x: batch_xs, y: batch_ys, learning_rate: lr(epoch)})\n",
        "        i = j\n",
        "\n",
        "    correct = (np.argmax(test_y, axis=1) == predicted_class)\n",
        "    acc = correct.mean() * 100\n",
        "    correct_numbers = correct.sum()\n",
        "    hours, rem = divmod(time() - epoch_start, 3600)\n",
        "    minutes, seconds = divmod(rem, 60)\n",
        "    mes = \"Epoch {} - accuracy: {:.2f}% ({}/{}) - time: {:0>2}:{:0>2}:{:05.2f}\"\n",
        "    print(mes.format((epoch + 1), acc, correct_numbers, len(test_x), int(hours), int(minutes), seconds))\n",
        "\n",
        "    if global_accuracy != 0 and global_accuracy < acc:\n",
        "        summary = tf.Summary(value=[tf.Summary.Value(tag=\"Accuracy/test\", simple_value=acc)])\n",
        "        train_writer.add_summary(summary, _global_step)\n",
        "        saver.save(sess, save_path=_SAVE_PATH, global_step=_global_step)\n",
        "        mes = \"This epoch received better accuracy: {:.2f} > {:.2f}. Saving session...\"\n",
        "        print(mes.format(acc, global_accuracy))\n",
        "        global_accuracy = acc\n",
        "\n",
        "    elif global_accuracy == 0:\n",
        "        global_accuracy = acc\n",
        "\n",
        "    print(\"###########################################################################################################\")\n",
        "\n",
        "def main():\n",
        "    train_start = time()\n",
        "\n",
        "    for i in range(_EPOCH):\n",
        "        print(\"Epoch: {}/{}\".format((i + 1), _EPOCH))\n",
        "        train(i)\n",
        "\n",
        "    hours, rem = divmod(time() - train_start, 3600)\n",
        "    minutes, seconds = divmod(rem, 60)\n",
        "    mes = \"Best accuracy per session: {:.2f}, time: {:0>2}:{:0>2}:{:05.2f}\"\n",
        "    print(mes.format(global_accuracy, int(hours), int(minutes), seconds))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "sess.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install include"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwdXpo3tTqeW",
        "outputId": "49862f68-4bff-44cf-e317-edc8bb572cc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting include\n",
            "  Downloading include-0.2.2-py2.py3-none-any.whl (14 kB)\n",
            "Installing collected packages: include\n",
            "Successfully installed include-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7oQp5sSMU2jA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}